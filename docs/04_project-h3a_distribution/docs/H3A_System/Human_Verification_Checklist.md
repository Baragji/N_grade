# Human Verification Checklist (H3A System)

**Version:** 1.0.0  
**Last Updated:** 2025-01-01  
**Status:** Production-Ready âœ…

---

## ğŸ“‹ Table of Contents

1. [Overview](#overview)
2. [When to Use This Checklist](#when-to-use-this-checklist)
3. [Prerequisites](#prerequisites)
4. [The 8-Point Verification Checklist](#the-8-point-verification-checklist)
5. [Quick Reference Card](#quick-reference-card)
6. [Decision Matrix](#decision-matrix)
7. [Common Issues & Solutions](#common-issues--solutions)
8. [Appendix: Verification Commands](#appendix-verification-commands)

---

## Overview

### Purpose

This checklist is your **final quality gate (G3)** before code reaches production. While the Validator agent has already performed technical verification (10-point checklist at G2), **you provide the human judgment** that AI cannot:

- âœ… **Intent alignment** - Does this solve the actual business problem?
- âœ… **User impact** - Will this improve or harm user experience?
- âœ… **Risk assessment** - What could go wrong in production?
- âœ… **Gut check** - Does this "feel right"?

**You are the last line of defense.** Trust your instincts.

---

### What You're NOT Doing

This is not a deep code review. You're doing a **spot check** to verify:

- âŒ **Not:** Reading every line of code
- âŒ **Not:** Re-running all quality gates (Validator did that)
- âŒ **Not:** Writing tests yourself
- âŒ **Not:** Fixing bugs

**You're verifying the Validator's work** and making a production deployment decision.

---

### Time Investment

**Target:** 10-15 minutes per task

| Activity | Time |
|----------|------|
| Read Validator report | 2 min |
| Run spot checks | 5 min |
| Review changed code | 3-5 min |
| Make decision + document | 2-3 min |

**If taking >20 minutes:** Task is too complex, send back to Planner for decomposition.

---

## When to Use This Checklist

**Trigger:** Validator completes G2 gate with "PASS" decision and writes `SESSION_HANDOFF.json` with `"handoff_to": "human"`.

**You verify:**
```bash
cat state/GATES_LEDGER.md | grep "G2:"
```

**Expected:**
```markdown
## Gate: G2 (Validation)
- **Status:** âœ… PASSED
- **Checklist:** 10/10 checks passed
- **Decision:** Approved for G3 gate
```

**If G2 â‰  PASSED:** Stop! Don't proceed to G3 until G2 issues resolved.

---

## Prerequisites

Before starting checklist, gather these resources:

### 1. **Context Files**

```bash
# Navigate to run directory
cd runs/<run-id>/

# Key files to review
cat validator_report.json     # Validator's findings
cat state/CURRENT_TASK.json   # Original requirements
cat executor_report.json      # Implementation details
```

### 2. **Changed Files**

```bash
# See what code was modified
git diff main...HEAD

# Or view specific files
git diff main...HEAD src/models/user.py
```

### 3. **Test Environment**

```bash
# Ensure you're in correct environment
python --version  # Should match project requirements
pip list | grep pytest

# Fresh shell (no cached modules)
```

### 4. **Validator Evidence**

```bash
# Check Validator's verification artifacts
ls -lh artifacts/validator/

# Key artifacts:
# - check_01_tdd_compliance.txt
# - check_02_test_quality.txt
# - check_03_coverage.txt
# - ... (10 total)
```

---

## The 8-Point Verification Checklist

### âœ… Point 1: Requirements Met

**Question:** Does the implementation solve the original problem?

**How to Verify:**

1. **Read original requirement:**
   ```bash
   cat state/CURRENT_TASK.json | jq '.goal'
   ```
   
   Example: `"Add email validation to User model"`

2. **Check implementation:**
   ```bash
   # Find the relevant code
   grep -n "email" src/models/user.py
   
   # Or view entire file
   cat src/models/user.py
   ```

3. **Spot check:**
   - Is there a `validate_email()` function or similar?
   - Does it check for basic email format (`@`, domain)?
   - Does it return clear success/failure indication?

**Pass Criteria:**
- âœ… Core functionality present
- âœ… Matches what was requested in `CURRENT_TASK.json`
- âœ… No obvious omissions

**Fail Criteria:**
- âŒ Feature not implemented
- âŒ Completely wrong approach (e.g., asked for validation, got formatting)
- âŒ Partial implementation (validates format but not domain)

**Time:** 2 minutes

---

### âœ… Point 2: Tests Pass

**Question:** Do all tests pass when I run them myself?

**How to Verify:**

```bash
# Run full test suite
pytest tests/ -v

# Expected output:
# ===== X passed in Y.YYs =====
# (No failures, no errors)
```

**Pass Criteria:**
- âœ… All tests pass (0 failures)
- âœ… No errors or crashes
- âœ… No warnings about deprecated APIs

**Fail Criteria:**
- âŒ Any test fails
- âŒ Tests crash with exception
- âŒ Tests hang (timeout after 60s)

**Troubleshooting:**

| Issue | Likely Cause | Fix |
|-------|--------------|-----|
| `ModuleNotFoundError` | Missing dependency | `pip install -r requirements.txt` |
| Tests fail but Validator said pass | Environment difference | Check Python version, dependencies |
| Tests hang | Infinite loop or network call | Kill tests, investigate hanging test |

**Time:** 2 minutes

---

### âœ… Point 3: No Broken Features

**Question:** Did this change break anything that previously worked?

**How to Verify:**

**Option A: Quick Smoke Test (Preferred)**

If your project has a smoke test:
```bash
# Run smoke test suite (fast, critical paths only)
pytest tests/smoke/ -v

# Or run specific integration test
pytest tests/test_integration.py -v
```

**Option B: Manual Spot Check**

If no smoke tests, manually test key workflows:

Example (for web app):
```bash
# Start app
python -m src.app

# Test in browser/curl:
# - Homepage loads
# - Login works
# - Critical feature works
```

**Pass Criteria:**
- âœ… Core functionality still works
- âœ… No obvious regressions
- âœ… App still starts/runs

**Fail Criteria:**
- âŒ Previously working feature now broken
- âŒ App won't start
- âŒ Critical path fails

**Time:** 3 minutes

---

### âœ… Point 4: Code Readable

**Question:** Can I understand what this code does without excessive effort?

**How to Verify:**

```bash
# View changed files
git diff main...HEAD src/

# Focus on new/modified functions
```

**Check for:**

1. **Clear function names**
   ```python
   # âœ… Good
   def validate_email_format(email: str) -> bool:
   
   # âŒ Bad
   def check(e):
   ```

2. **Descriptive variable names**
   ```python
   # âœ… Good
   email_pattern = r"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$"
   
   # âŒ Bad
   p = r"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$"
   ```

3. **Reasonable complexity**
   ```python
   # âœ… Good (simple, linear logic)
   def validate_email(email: str) -> bool:
       if not email:
           return False
       if "@" not in email:
           return False
       return re.match(EMAIL_PATTERN, email) is not None
   
   # âŒ Bad (nested, hard to follow)
   def validate_email(email: str) -> bool:
       if email:
           if "@" in email:
               if "." in email.split("@")[1]:
                   if len(email) > 5:
                       if not email.startswith("."):
                           return True
       return False
   ```

4. **Comments where needed**
   ```python
   # âœ… Good (complex regex explained)
   # Matches: user@domain.com, first.last@company.co.uk
   # Does not match: @domain, user@, user@domain
   EMAIL_PATTERN = r"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$"
   ```

**Pass Criteria:**
- âœ… Function names describe purpose
- âœ… Can follow logic in 30-60 seconds
- âœ… No excessive nesting (< 4 levels)
- âœ… Complex parts have comments

**Fail Criteria:**
- âŒ Cannot understand what code does
- âŒ Variables named `x`, `data`, `tmp`
- âŒ 100+ line function with no structure
- âŒ No comments on complex logic

**Time:** 3 minutes

---

### âœ… Point 5: Security Check

**Question:** Is there any obvious security risk?

**How to Verify:**

**Check 1: Validator's Security Scan**
```bash
cat artifacts/validator/check_05_security.txt

# Expected: 0 critical, 0 high issues
```

**Check 2: Manual Code Review**

Look for common security issues:

| Risk | Example | Fix |
|------|---------|-----|
| **Secrets in code** | `API_KEY = "sk-abc123"` | Use environment variables |
| **SQL injection** | `f"SELECT * FROM users WHERE email='{email}'"` | Use parameterized queries |
| **Command injection** | `os.system(f"ping {user_input}")` | Validate input, use safe APIs |
| **Path traversal** | `open(f"files/{user_file}")` | Validate path, use safe join |
| **Missing input validation** | `age = int(request.args['age'])` | Validate, handle exceptions |

**Check 3: Look at Test Coverage**

```bash
cat artifacts/validator/check_03_coverage.txt | grep -A 5 "missing"

# Are security-critical paths covered?
```

**Pass Criteria:**
- âœ… Validator security scan clean (0 critical/high)
- âœ… No secrets in code
- âœ… User input validated before use
- âœ… No dangerous patterns (SQL injection, command injection)

**Fail Criteria:**
- âŒ ANY critical or high security issue
- âŒ Secrets, API keys, passwords in code
- âŒ User input used directly in SQL, shell commands, file paths
- âŒ Missing validation on untrusted input

**If ANY security issue found:** âŒ REJECT immediately, send back to Executor

**Time:** 2 minutes

---

### âœ… Point 6: Documentation Updated

**Question:** If someone reads the docs, will they understand this feature?

**How to Verify:**

**Check 1: Docstrings**

```bash
# View new functions
git diff main...HEAD src/ | grep -A 10 "def "
```

Example:
```python
# âœ… Good
def validate_email(email: str) -> bool:
    """
    Validate email format according to RFC 5322 (simplified).
    
    Args:
        email: Email address to validate
        
    Returns:
        True if valid format, False otherwise
        
    Example:
        >>> validate_email("user@example.com")
        True
        >>> validate_email("invalid")
        False
    """
    ...

# âŒ Bad (no docstring)
def validate_email(email: str) -> bool:
    ...
```

**Check 2: README/User Docs**

If this is a user-facing feature:
```bash
# Check if README mentions it
grep -i "email" README.md

# Check if CHANGELOG updated
cat CHANGELOG.md | head -20
```

**Check 3: Definition of Done**

```bash
cat state/CURRENT_TASK.json | jq '.definition_of_done[] | select(.item | contains("documentation"))'
```

**Pass Criteria:**
- âœ… New functions have docstrings
- âœ… Public API documented
- âœ… README updated if user-facing
- âœ… CHANGELOG has entry (if project uses it)

**Conditional Pass:**
- âš ï¸ Internal functions without docstrings (OK if names are self-documenting)
- âš ï¸ No README update for internal refactoring (OK if no user impact)

**Fail Criteria:**
- âŒ Public API has no docstrings
- âŒ User-facing feature not mentioned in README
- âŒ Complex logic without any explanation

**Time:** 2 minutes

---

### âœ… Point 7: Evidence Complete

**Question:** Can I audit this task in 6 months if there's an issue?

**How to Verify:**

**Check 1: Evidence Log**

```bash
cat state/EVIDENCE_LOG.md | grep "T001"

# Expected: All artifacts listed with SHA-256 hashes
```

Example:
```markdown
## Task: T001 - Add email validation

### Executor Evidence
- `artifacts/executor/test_output_red.txt` â†’ `sha256:a3f4b2c...`
- `artifacts/executor/test_output_green.txt` â†’ `sha256:e8d7a1f...`
- `artifacts/executor/test_output_full_suite.txt` â†’ `sha256:c2b5e9d...`
... (7 total)

### Validator Evidence
- `artifacts/validator/check_01_tdd_compliance.txt` â†’ `sha256:f1e3d8a...`
- `artifacts/validator/check_02_test_quality.txt` â†’ `sha256:b9c4e2f...`
... (10+ total)
```

**Check 2: Artifacts Exist**

```bash
# Verify files actually exist and match hashes
for file in artifacts/executor/*.txt; do
    echo "Checking $file"
    ls -lh "$file"
done
```

**Check 3: Gates Ledger Complete**

```bash
tail -50 state/GATES_LEDGER.md

# Should show:
# - G0: PASSED (Planner)
# - G1: PASSED (Executor)
# - G2: PASSED (Validator)
# - G3: PENDING (awaiting your decision)
```

**Pass Criteria:**
- âœ… All evidence files listed in EVIDENCE_LOG.md
- âœ… All files exist and have hashes
- âœ… GATES_LEDGER shows full progression (G0 â†’ G1 â†’ G2)
- âœ… No gaps in audit trail

**Fail Criteria:**
- âŒ Evidence files missing
- âŒ EVIDENCE_LOG.md incomplete
- âŒ Gates skipped (e.g., G0 â†’ G2 without G1)

**Time:** 2 minutes

---

### âœ… Point 8: Intent Alignment

**Question:** Is this what we actually needed, or did scope creep occur?

**How to Verify:**

**Check 1: Compare Goal vs Implementation**

```bash
# Original goal
cat state/CURRENT_TASK.json | jq '.goal'
# "Add email validation to User model"

# What was actually done
cat executor_report.json | jq '.summary'
# Check: Did they ONLY add email validation, or also add password validation, username validation, etc.?
```

**Check 2: Files Modified**

```bash
# What files were supposed to change
cat state/CURRENT_TASK.json | jq '.scope.files_to_modify'
# ["src/models/user.py", "tests/test_user.py"]

# What files actually changed
git diff main...HEAD --name-only
# Should match exactly (or be subset)
```

**Check 3: LOC Estimate**

```bash
# Estimated lines of code
cat state/CURRENT_TASK.json | jq '.scope.estimated_loc'
# 120

# Actual lines changed
git diff main...HEAD --stat
# Should be close (within 50% margin)
```

**Scope Creep Examples:**

| Requested | What Was Delivered | Scope Creep? |
|-----------|-------------------|--------------|
| Add email validation | Email validation only | âœ… Perfect |
| Add email validation | Email validation + password validation + username validation | âŒ YES (did too much) |
| Add email validation | Email validation + fixed existing bug | âš ï¸ MAYBE (ask: was bug related?) |
| Add email validation | Refactored entire User model | âŒ YES (changed too much) |

**Pass Criteria:**
- âœ… Implementation matches goal
- âœ… Files changed match scope
- âœ… LOC within reasonable range (Â±50% of estimate)
- âœ… No unexpected features added

**Conditional Pass:**
- âš ï¸ Small related fix (e.g., fixed typo in adjacent line) - OK
- âš ï¸ Slightly over LOC estimate but justified - OK

**Fail Criteria:**
- âŒ Extra features added without approval
- âŒ Files modified outside scope
- âŒ 3x+ LOC estimate (task should have been decomposed)

**Time:** 2 minutes

---

## Quick Reference Card

### Print & Pin This to Your Wall ğŸ“Œ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         H3A HUMAN VERIFICATION CHECKLIST (G3 Gate)              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  âœ… 1. REQUIREMENTS MET                                         â”‚
â”‚     Does code solve original problem?                           â”‚
â”‚     â†’ Check: state/CURRENT_TASK.json vs implementation          â”‚
â”‚                                                                 â”‚
â”‚  âœ… 2. TESTS PASS                                               â”‚
â”‚     Do tests pass when I run them?                              â”‚
â”‚     â†’ Run: pytest tests/ -v                                     â”‚
â”‚                                                                 â”‚
â”‚  âœ… 3. NO BROKEN FEATURES                                       â”‚
â”‚     Did this break anything?                                    â”‚
â”‚     â†’ Run: pytest tests/smoke/ OR manual smoke test             â”‚
â”‚                                                                 â”‚
â”‚  âœ… 4. CODE READABLE                                            â”‚
â”‚     Can I understand what this does?                            â”‚
â”‚     â†’ Check: git diff main...HEAD src/                          â”‚
â”‚                                                                 â”‚
â”‚  âœ… 5. SECURITY CHECK                                           â”‚
â”‚     Any obvious security risks?                                 â”‚
â”‚     â†’ Check: artifacts/validator/check_05_security.txt          â”‚
â”‚     â†’ Look for: secrets, SQL injection, missing validation      â”‚
â”‚                                                                 â”‚
â”‚  âœ… 6. DOCUMENTATION UPDATED                                    â”‚
â”‚     Are new functions documented?                               â”‚
â”‚     â†’ Check: Docstrings, README, CHANGELOG                      â”‚
â”‚                                                                 â”‚
â”‚  âœ… 7. EVIDENCE COMPLETE                                        â”‚
â”‚     Can I audit this later?                                     â”‚
â”‚     â†’ Check: state/EVIDENCE_LOG.md, GATES_LEDGER.md            â”‚
â”‚                                                                 â”‚
â”‚  âœ… 8. INTENT ALIGNMENT                                         â”‚
â”‚     Is this what we needed (no scope creep)?                    â”‚
â”‚     â†’ Check: goal vs implementation, files changed, LOC         â”‚
â”‚                                                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  DECISION:                                                      â”‚
â”‚  [ ] âœ… APPROVE (8/8 passed)                                    â”‚
â”‚  [ ] âš ï¸  CONDITIONAL APPROVAL (6-7/8 passed, minor fixes)       â”‚
â”‚  [ ] âŒ REJECT (<6/8 passed OR any security issue)             â”‚
â”‚                                                                 â”‚
â”‚  Time: 10-15 minutes per task                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Decision Matrix

### How to Make G3 Decision

Use this flowchart:

```
START: Review validator_report.json
  â†“
Q1: Did Validator report "PASS"?
  â”œâ”€ NO â†’ âŒ REJECT (don't proceed to G3 if G2 failed)
  â””â”€ YES â†’ Continue
  â†“
Q2: Did ANY of 8 human checks FAIL?
  â”œâ”€ YES â†’ Go to Q3
  â””â”€ NO â†’ âœ… APPROVE (all checks passed)
  â†“
Q3: Was the failure Point 5 (Security)?
  â”œâ”€ YES â†’ âŒ REJECT (security always blocks)
  â””â”€ NO â†’ Go to Q4
  â†“
Q4: How many checks failed?
  â”œâ”€ 1-2 checks â†’ Go to Q5
  â”œâ”€ 3-4 checks â†’ âŒ REJECT (too many issues)
  â””â”€ 5+ checks â†’ âŒ REJECT (fundamental problems)
  â†“
Q5: Are failures cosmetic (docs, style)?
  â”œâ”€ YES â†’ âš ï¸ CONDITIONAL APPROVAL (require minor fixes)
  â””â”€ NO â†’ Go to Q6
  â†“
Q6: Are failures fixable in <15 minutes?
  â”œâ”€ YES â†’ âš ï¸ CONDITIONAL APPROVAL (require fixes)
  â””â”€ NO â†’ âŒ REJECT (send back to Executor)
  â†“
END: Document decision in state/GATES_LEDGER.md
```

---

### Decision Outcomes

#### âœ… APPROVE

**When:**
- 8/8 checks passed
- OR: 7/8 passed with cosmetic issue (missing comment, typo)

**Action:**
```bash
# Update gates ledger
echo "## Gate: G3 (Production-Ready)
- **Status:** âœ… APPROVED
- **Timestamp:** $(date -u +"%Y-%m-%dT%H:%M:%SZ")
- **Approver:** [Your Name]
- **Checklist:** 8/8 passed
- **Next Action:** Deploy to production
" >> state/GATES_LEDGER.md

# Create approval report
cat > g3_approval_report.json << EOF
{
  "gate": "G3",
  "decision": "APPROVED",
  "timestamp": "$(date -u +"%Y-%m-%dT%H:%M:%SZ")",
  "approver": "[Your Name]",
  "checklist_passed": 8,
  "checklist_failed": 0,
  "deployment_approved": true
}
EOF
```

**Next Steps:**
- Merge PR
- Deploy to production
- If ROADMAP has more tasks â†’ Return to Planner for next task

---

#### âš ï¸ CONDITIONAL APPROVAL

**When:**
- 6-7/8 checks passed
- Failures are minor and fixable in <15 minutes
- No security issues

**Action:**
```bash
# Document required fixes
cat > state/G3_CONDITIONAL_FIXES.md << 'EOF'
## Conditional Approval - Minor Fixes Required

**Task:** T001 - Add email validation
**Approver:** [Your Name]
**Date:** 2025-01-01

### Issues Found:
1. âŒ Point 6 (Documentation): Missing docstring for `validate_email()`
2. âŒ Point 8 (Intent): Modified 3 files instead of 2 (added validation to Admin model without approval)

### Required Fixes:
1. Add docstring to `validate_email()` with description, args, returns, example
2. Remove validation from Admin model (out of scope) OR get approval to expand scope

### Deadline:
- 1 iteration (max 30 minutes)
- After fixes: I will re-verify Points 6 & 8 only

### Re-approval Process:
- Executor fixes issues
- Executor updates executor_report.json
- Human re-checks failed points
- If fixed â†’ âœ… APPROVE
- If not fixed â†’ âŒ REJECT
EOF

# Update gates ledger
echo "## Gate: G3 (Production-Ready)
- **Status:** âš ï¸ CONDITIONAL APPROVAL
- **Timestamp:** $(date -u +"%Y-%m-%dT%H:%M:%SZ")
- **Approver:** [Your Name]
- **Checklist:** 6/8 passed
- **Fixes Required:** See state/G3_CONDITIONAL_FIXES.md
" >> state/GATES_LEDGER.md
```

**Next Steps:**
- Send G3_CONDITIONAL_FIXES.md to Executor
- Wait for fixes
- Re-verify only failed points
- Make final decision (APPROVE or REJECT)

---

#### âŒ REJECT

**When:**
- <6/8 checks passed
- OR: ANY security issue (regardless of other checks)
- OR: Fundamental problem (wrong approach, scope creep)

**Action:**
```bash
# Document rejection
cat > state/G3_REJECTION_REPORT.md << 'EOF'
## G3 Rejection Report

**Task:** T001 - Add email validation
**Approver:** [Your Name]
**Date:** 2025-01-01

### Rejection Reason:
âŒ Point 5 (Security): Found SQL injection vulnerability in email lookup

### Details:
File: src/models/user.py, Line 45
```python
query = f"SELECT * FROM users WHERE email = '{email}'"
cursor.execute(query)  # âŒ SQL injection risk
```

### Impact:
CRITICAL - Attacker can inject SQL: `' OR '1'='1`

### Required Action:
Return to Executor for complete rewrite using parameterized queries.

### Next Steps:
1. Executor must use parameterized queries (SQLAlchemy ORM or psycopg2 params)
2. Validator must re-verify security (check_05)
3. Human must re-review (full 8-point checklist)
EOF

# Update gates ledger
echo "## Gate: G3 (Production-Ready)
- **Status:** âŒ REJECTED
- **Timestamp:** $(date -u +"%Y-%m-%dT%H:%M:%SZ")
- **Approver:** [Your Name]
- **Checklist:** 4/8 passed (Security FAILED)
- **Reason:** Critical security vulnerability (SQL injection)
- **Next Action:** Return to Executor for fix
" >> state/GATES_LEDGER.md
```

**Next Steps:**
- Send rejection report to Executor
- Executor fixes issue, re-submits
- Validator re-runs ALL checks (full G2 verification)
- Human re-runs ALL checks (full 8-point checklist)

**Iteration Limit:** Max 2 rejection cycles, then escalate to tech lead

---

## Common Issues & Solutions

### Issue 1: "Tests pass for Validator but fail for me"

**Symptom:**
```bash
pytest tests/
# FAILED tests/test_user.py::test_email_validation
```

But `artifacts/validator/check_02_test_quality.txt` shows all tests passing.

**Possible Causes:**

| Cause | How to Diagnose | Fix |
|-------|----------------|-----|
| **Environment difference** | `python --version` | Match Python version to Validator's |
| **Missing dependency** | `pip list` vs `requirements.txt` | `pip install -r requirements.txt` |
| **Cached modules** | Old `.pyc` files | `find . -type d -name __pycache__ -exec rm -rf {} +` |
| **Test order dependency** | Run single test: `pytest tests/test_user.py::test_email_validation` | Fix test isolation |
| **Timing issue** | Test uses `time.time()` without mocking | Mock time/random in tests |

**Resolution:**
1. Try running in fresh virtual environment
2. If still fails â†’ Validator had environment issue, trust your result
3. If passes in fresh env â†’ Your environment needs cleanup

---

### Issue 2: "Code works but I don't understand it"

**Symptom:** Point 4 (Code Readable) fails - code is too complex or unclear.

**Decision Framework:**

**Complexity Assessment:**

```python
# Example 1: Moderately complex but reasonable
def validate_email(email: str) -> bool:
    """Validate email format."""
    if not email or len(email) > 254:
        return False
    
    local, _, domain = email.partition("@")
    if not local or not domain:
        return False
    
    if not re.match(r"^[a-zA-Z0-9._%+-]+$", local):
        return False
    
    if not re.match(r"^[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$", domain):
        return False
    
    return True

# âœ… PASS: Linear logic, each check is clear
```

```python
# Example 2: Too complex
def validate_email(email: str) -> bool:
    return bool(email and len(email) <= 254 and (lambda x: x[0] and x[1] if len(x := email.partition("@")) == 3 else False)() and all([re.match(r"^[a-zA-Z0-9._%+-]+$", email.split("@")[0]), re.match(r"^[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$", email.split("@")[1])]))

# âŒ FAIL: One-liner with lambdas, hard to debug
```

**Your Decision:**

- **If you can follow it in 60 seconds:** âœ… PASS (even if not elegant)
- **If you need >2 minutes to understand:** âš ï¸ CONDITIONAL APPROVAL (request comments or refactor)
- **If you cannot understand it:** âŒ REJECT (request simplification)

**When in doubt:** Ask yourself "Can a junior developer maintain this?" If NO â†’ send back.

---

### Issue 3: "Scope creep detected but changes look good"

**Symptom:** Point 8 (Intent Alignment) fails - Extra features added, but they're useful.

**Example:**
- **Requested:** Add email validation
- **Delivered:** Email validation + password validation + username validation

**Decision Framework:**

**Ask:**

1. **Were extra features necessary?**
   - If YES (e.g., email validation requires username validation) â†’ âœ… APPROVE
   - If NO (could work without them) â†’ Continue

2. **Do extra features add risk?**
   - If YES (touch critical code, security-sensitive) â†’ âŒ REJECT
   - If NO (isolated, well-tested) â†’ Continue

3. **Was scope expansion documented?**
   - If YES (executor_report.json explains why) â†’ âš ï¸ CONDITIONAL APPROVAL
   - If NO (no justification) â†’ âŒ REJECT

4. **How much extra work was done?**
   - If <30% extra LOC â†’ âš ï¸ CONDITIONAL APPROVAL (accept but document)
   - If >50% extra LOC â†’ âŒ REJECT (should have been separate task)

**Best Practice:**
- âš ï¸ Accept scope expansion if it's small, low-risk, and documented
- âŒ Reject if it's large, risky, or unexplained
- Document in approval report: "Scope expanded to include X, accepted because Y"

---

### Issue 4: "Security scan found 'info' issues, do I care?"

**Symptom:** `check_05_security.txt` shows warnings but no critical/high issues.

**Example:**
```
INFO: Line 23: Use of assert statement
INFO: Line 45: Use of eval (but input is constant string)
```

**Decision Framework:**

| Severity | Action | Rationale |
|----------|--------|-----------|
| **Critical** | âŒ REJECT | Remote code execution, SQL injection, authentication bypass |
| **High** | âŒ REJECT | Data exposure, privilege escalation, DoS |
| **Medium** | âš ï¸ CONDITIONAL | Depends on context (e.g., weak crypto in test file is OK) |
| **Low** | âœ… PASS | Create follow-up ticket but don't block deployment |
| **Info** | âœ… PASS | Ignore (informational only) |

**For INFO/LOW issues:**
- âœ… Pass G3 gate
- Create follow-up ticket for technical debt
- Document in approval report

---

### Issue 5: "I'm 50/50 on whether to approve"

**Symptom:** Some checks pass, some are borderline, overall uncertain.

**Decision Framework:**

**When uncertain, ask:**

1. **"If this breaks in production, how bad is the impact?"**
   - Catastrophic (data loss, security breach) â†’ âŒ REJECT
   - Bad (service down, users can't work) â†’ âŒ REJECT
   - Annoying (UI bug, wrong error message) â†’ âš ï¸ CONDITIONAL
   - Negligible (cosmetic issue) â†’ âœ… APPROVE

2. **"Can I explain this decision to my manager?"**
   - If NO â†’ âŒ REJECT (trust your gut)
   - If YES â†’ Continue

3. **"Have I seen the Executor deliver quality work before?"**
   - If YES (established trust) â†’ âš ï¸ CONDITIONAL (give benefit of doubt)
   - If NO (first task or history of issues) â†’ âŒ REJECT (hold to higher bar)

**Default Position:**
- **When uncertain:** âš ï¸ CONDITIONAL APPROVAL with specific fixes
- **If still uncertain after fixes:** âŒ REJECT (better safe than sorry)
- **Never approve if you're uncomfortable with security or correctness**

**Escalation:**
If you're stuck, escalate to:
- Tech lead (for architecture decisions)
- Security team (for security questions)
- Product manager (for scope/requirements questions)

---

## Appendix: Verification Commands

### Quick Command Reference

```bash
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# SETUP (run once)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Navigate to run directory
cd runs/<run-id>/

# Activate virtual environment (if using)
source venv/bin/activate

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# POINT 1: REQUIREMENTS MET
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Read original goal
cat state/CURRENT_TASK.json | jq '.goal'

# Check what was implemented
git diff main...HEAD src/ | less

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# POINT 2: TESTS PASS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Run all tests
pytest tests/ -v

# Run with coverage
pytest --cov=src --cov-report=term-missing

# Run specific test file
pytest tests/test_user.py -v

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# POINT 3: NO BROKEN FEATURES
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Run smoke tests (if available)
pytest tests/smoke/ -v

# Or run integration tests
pytest tests/integration/ -v

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# POINT 4: CODE READABLE
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# View all changed code
git diff main...HEAD src/ | less

# View specific file
git diff main...HEAD src/models/user.py

# Check complexity (if using radon)
radon cc src/models/user.py -a

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# POINT 5: SECURITY CHECK
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Read Validator's security scan
cat artifacts/validator/check_05_security.txt

# Re-run security scan yourself
bandit -r src/ -f txt

# Check for secrets
git diff main...HEAD | grep -i "api_key\|password\|secret\|token"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# POINT 6: DOCUMENTATION UPDATED
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Check docstrings in changed code
git diff main...HEAD src/ | grep -A 10 "def "

# Check README mentions feature
grep -i "email\|validation" README.md

# Check CHANGELOG
head -20 CHANGELOG.md

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# POINT 7: EVIDENCE COMPLETE
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# List all evidence artifacts
ls -lh artifacts/executor/
ls -lh artifacts/validator/

# Check evidence log
cat state/EVIDENCE_LOG.md | grep "T001"

# Verify gate progression
cat state/GATES_LEDGER.md

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# POINT 8: INTENT ALIGNMENT
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Compare goal vs summary
cat state/CURRENT_TASK.json | jq '.goal'
cat executor_report.json | jq '.summary'

# Check files changed
cat state/CURRENT_TASK.json | jq '.scope.files_to_modify'
git diff main...HEAD --name-only

# Check LOC estimate vs actual
cat state/CURRENT_TASK.json | jq '.scope.estimated_loc'
git diff main...HEAD --stat

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# DECISION DOCUMENTATION
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Approve
echo "G3: âœ… APPROVED" >> state/GATES_LEDGER.md

# Conditional approval
echo "G3: âš ï¸ CONDITIONAL APPROVAL" >> state/GATES_LEDGER.md
# (create state/G3_CONDITIONAL_FIXES.md with fix list)

# Reject
echo "G3: âŒ REJECTED" >> state/GATES_LEDGER.md
# (create state/G3_REJECTION_REPORT.md with details)
```

---

## Summary

### The 8-Point Checklist (Memorize This)

1. **Requirements Met** - Does it solve the problem?
2. **Tests Pass** - Do tests pass when I run them?
3. **No Broken Features** - Did this break anything?
4. **Code Readable** - Can I understand it?
5. **Security Check** - Any obvious risks?
6. **Documentation Updated** - Are new functions documented?
7. **Evidence Complete** - Can I audit this later?
8. **Intent Alignment** - Is this what we needed (no scope creep)?

### Decision Rules (Memorize This)

- **8/8 passed** â†’ âœ… APPROVE
- **6-7/8 passed, minor issues** â†’ âš ï¸ CONDITIONAL APPROVAL
- **<6/8 passed** â†’ âŒ REJECT
- **ANY security issue** â†’ âŒ REJECT (regardless of other checks)

### Time Budget

- **Target:** 10-15 minutes per task
- **Maximum:** 20 minutes
- **If taking longer:** Task too complex, send back to Planner

---

## Version History

- **v1.0.0** (2025-01-01): Initial production release

---

**You are the final guardian before production. Trust your judgment! ğŸ›¡ï¸**

For bridge protocol, see [Human_Bridge_Protocol.md](./Human_Bridge_Protocol.md)  
For complete workflow, see [Quick_Start_Guide.md](./Quick_Start_Guide.md)